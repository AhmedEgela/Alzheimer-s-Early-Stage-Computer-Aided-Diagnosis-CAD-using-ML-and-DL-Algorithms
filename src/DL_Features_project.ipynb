{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MF1ssBKfj6K"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttVyf2Cypk8w"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7vxKqDTfguD",
        "outputId": "b1896a9b-e315-45df-d988-6f68ff6af240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the file name and extraction path\n",
        "zip_file_path = \"/content/data.zip\"\n",
        "extract_path = \"/content/\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yXnPiQRph6sH",
        "outputId": "ece7d2fa-4714-4d11-8467-564b9ce2e2a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4XAbbeniEMm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications import ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AihsOAzOfLuj"
      },
      "source": [
        "### **InceptionV3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da9EgwvzfE0j",
        "outputId": "4fbfc865-2e75-4a79-8bcb-e93bfeeef255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10240 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 931ms/step\n",
            "Success FE, CSV file has been generated as 'sample_features_inceptionV3.csv'.\n"
          ]
        }
      ],
      "source": [
        "Path_inceptionV3 = \"/content/All\"\n",
        "# Data generator for the dataset\n",
        "datagen_IV3 = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.inception_v3.preprocess_input\n",
        ")\n",
        "generator_IV3 = datagen_IV3.flow_from_directory( # training or testing\n",
        "    Path_inceptionV3,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # in order\n",
        ")\n",
        "# Load the Inception V3 model for feature extraction\n",
        "base_model_IV3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  # Updated input shape\n",
        "base_model_IV3.trainable = False  # Explicitly freeze the base model\n",
        "model_IV3 = Model(inputs=base_model_IV3.input, outputs=base_model_IV3.output)\n",
        "# Function to extract features\n",
        "def extract_features(generator, model):\n",
        "    features = model.predict(generator, verbose=1) ## prints progress during prediction\n",
        "    labels = generator.classes\n",
        "    return features, labels\n",
        "# Extract features from the dataset\n",
        "features_IV3, labels_IV3 = extract_features(generator_IV3, model_IV3)\n",
        "# Flatten the features\n",
        "features_flat_IV3 = features_IV3.reshape(features_IV3.shape[0], -1)\n",
        "# Combine features and labels\n",
        "data_IV3 = np.column_stack((features_flat_IV3, labels_IV3))\n",
        "# Create a DataFrame\n",
        "columns = [f'feature_{i}' for i in range(features_flat_IV3.shape[1])] + ['class']\n",
        "df_IV3 = pd.DataFrame(data_IV3, columns=columns)\n",
        "df_IV3.to_csv('features_inceptionV3.csv', index=False)# Save to CSV\n",
        "print(\"Success FE, CSV file has been generated as 'features_inceptionV3.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm8dgrVYg0OK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I076mrYgg2Ox"
      },
      "source": [
        "### **EfficientNetV2B0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG7_7lgBg9By",
        "outputId": "a126d631-242a-4fb8-d7ae-230c9d4cdb29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10240 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 584ms/step\n",
            "Success FE, CSV file has been generated as 'sample_features_efficientnet.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Path to the dataset\n",
        "Path_EfficientNetV2B0 = \"/content/All\"\n",
        "# Data generator for the dataset\n",
        "datagen_ENV2 = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "# Create a generator for the data\n",
        "generator_ENV2 = datagen_ENV2.flow_from_directory(\n",
        "    Path_EfficientNetV2B0,\n",
        "    target_size=(128, 128),  # Updated target size to match Code 1\n",
        "    batch_size=64,          # Updated batch size to match Code 1\n",
        "    class_mode='categorical',\n",
        "    shuffle=False           # Ensure order consistency for feature extraction\n",
        ")\n",
        "# Load the EfficientNetV2 B0 model for feature extraction\n",
        "base_model_ENV2 = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  # Updated input shape\n",
        "base_model_ENV2.trainable = False  # Explicitly freeze the base model\n",
        "model_EfficientNetV2B0 = Model(inputs=base_model_ENV2.input, outputs=base_model_ENV2.output)\n",
        "# Function to extract features\n",
        "def extract_features(generator, model):\n",
        "    features = model.predict(generator, verbose=1) ## prints progress during prediction\n",
        "    labels = generator.classes\n",
        "    return features, labels\n",
        "features_ENV2, labels_ENV2 = extract_features(generator_ENV2,model_EfficientNetV2B0) # Extract features from the dataset\n",
        "# Flatten the features\n",
        "features_flat_ENV2 = features_ENV2.reshape(features_ENV2.shape[0], -1) # A 1D array is often required by machine learning algorithms. A -1 flattens the features into a 1D array.\n",
        "data_ENV2 = np.column_stack((features_flat_ENV2, labels_ENV2)) # Combine features and labels\n",
        "# Create a DataFrame\n",
        "columns = [f'feature_{i}' for i in range(features_flat_ENV2.shape[1])] + ['class']\n",
        "df_ENV2 = pd.DataFrame(data_ENV2, columns=columns)\n",
        "\n",
        "df_ENV2.to_csv('features_efficientnet.csv', index=False) # Save to CSV\n",
        "print(\"Success FE, CSV file has been generated as 'features_efficientnet.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L66xX8mTg9Fi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USsNW-PBhD5X"
      },
      "source": [
        "### **VGG19**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn-LqkjqhEDD",
        "outputId": "779caa78-b270-4c82-c344-800d7869ac80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10240 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3148s\u001b[0m 10s/step\n",
            "Success FE CSV file has been generated as 'sample_features_vgg19.csv'.\n"
          ]
        }
      ],
      "source": [
        "Path_VGG19 = \"/content/All\" # Path to the dataset\n",
        "# Data generator for the dataset\n",
        "datagen_VGG19 = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.vgg19.preprocess_input\n",
        ")\n",
        "# Create a generator for the data\n",
        "generator_VGG19 = datagen_VGG19.flow_from_directory(\n",
        "    Path_VGG19,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False           # Ensure order consistency for feature extraction\n",
        ")\n",
        "# Load the VGG19 model for feature extraction\n",
        "base_model_VGG19 = VGG19(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  # Updated input shape\n",
        "base_model_VGG19.trainable = False  # Explicitly freeze the base model\n",
        "model_VGG19 = Model(inputs=base_model_VGG19.input, outputs=base_model_VGG19.output)\n",
        "# Function to extract features\n",
        "def extract_features(generator, model):\n",
        "    features = model.predict(generator, verbose=1)\n",
        "    labels = generator.classes\n",
        "    return features, labels\n",
        "features_VGG19, labels_VGG19 = extract_features(generator_VGG19, model_VGG19) # Extract features from the dataset\n",
        "features_flat_VGG19 = features_VGG19.reshape(features_VGG19.shape[0], -1) # Flatten the features\n",
        "data_VGG19 = np.column_stack((features_flat_VGG19, labels_VGG19)) # Combine features and labels\n",
        "# Create a DataFrame\n",
        "columns = [f'feature_{i}' for i in range(features_flat_VGG19.shape[1])] + ['class']\n",
        "df_VGG19 = pd.DataFrame(data_VGG19, columns=columns)\n",
        "\n",
        "df_VGG19.to_csv('sample_features_vgg19.csv', index=False) # Save to CSV\n",
        "print(\"Success FE CSV file has been generated as 'sample_features_vgg19.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQ-RSzvhFR-"
      },
      "source": [
        "### **ResNet18**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34vTw2aghFX0",
        "outputId": "d9ff23ec-eef9-4d76-8e94-253785a2730a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10240 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1794s\u001b[0m 1s/step\n"
          ]
        }
      ],
      "source": [
        "data_Path_ResNet18 = \"/content/All\" # Path to the dataset\n",
        "# Data generator for the dataset\n",
        "datagen_ResNet18 = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
        ")\n",
        "# Create a generator for the data\n",
        "generator_ResNet18 = datagen_ResNet18.flow_from_directory(\n",
        "    data_Path_ResNet18,\n",
        "    target_size=(224, 224),  # 224x224 input size\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Ensure order consistency for feature extraction\n",
        ")\n",
        "# Load the ResNet18 model for feature extraction\n",
        "base_model_ResNet18 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model_ResNet18 = Model(inputs=base_model_ResNet18.input, outputs=base_model_ResNet18.output)\n",
        "# Function to extract features\n",
        "def extract_features_ResNet18(generator, model):\n",
        "    features = model.predict(generator, verbose=1)\n",
        "    labels = generator.classes\n",
        "    return features, labels\n",
        "\n",
        "features_ResNet18, labels_ResNet18 = extract_features_ResNet18(generator_ResNet18, model_ResNet18) # Extract features from the dataset\n",
        "features_flat_ResNet18 = features_ResNet18.reshape(features_ResNet18.shape[0], -1) # Flatten the features\n",
        "data_ResNet18 = np.column_stack((features_flat_ResNet18, labels_ResNet18)) # Combine features and labels\n",
        "# Create a DataFrame\n",
        "columns_ResNet18 = [f'feature_{i}' for i in range(features_flat_ResNet18.shape[1])] + ['class']\n",
        "df_ResNet18 = pd.DataFrame(data_ResNet18, columns=columns_ResNet18)\n",
        "\n",
        "df_ResNet18.to_csv('features_resnet18.csv', index=False) # Save to CSV\n",
        "print(\"Feature extraction complete. CSV file has been generated as 'features_resnet18.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG8U1pj8hFaD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWBAVRfUhE9a"
      },
      "source": [
        "### **SqueezeNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T9DHlS0hFDb",
        "outputId": "081409c2-b92c-4542-fb3a-2f5f76413c71"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
            "100%|██████████| 4.78M/4.78M [00:01<00:00, 4.12MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_Path_SqueezeNet = \"/content/SampleAll\" # Path to the dataset\n",
        "# Transformations for the dataset\n",
        "transform_SqueezeNet = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # 224x224 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "# Custom Dataset class for SqueezeNet\n",
        "class CustomImageDataset_SqueezeNet(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_list = []\n",
        "        self.label_list = []\n",
        "        for label, sub_dir in enumerate(os.listdir(image_dir)):\n",
        "            class_dir = os.path.join(image_dir, sub_dir)\n",
        "            for img_file in os.listdir(class_dir):\n",
        "                self.image_list.append(os.path.join(class_dir, img_file))\n",
        "                self.label_list.append(label)\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.image_list[index]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.label_list[index]\n",
        "        return image, label\n",
        "# Data generator for the dataset\n",
        "dataset_SqueezeNet = CustomImageDataset_SqueezeNet(\n",
        "    image_dir=data_Path_SqueezeNet,\n",
        "    transform=transform_SqueezeNet\n",
        ")\n",
        "# Create a DataLoader for the data\n",
        "dataloader_SqueezeNet = DataLoader(\n",
        "    dataset_SqueezeNet,\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        ")\n",
        "# Load the SqueezeNet model for feature extraction\n",
        "base_model_SqueezeNet = models.squeezenet1_0(pretrained=True)\n",
        "model_SqueezeNet = nn.Sequential(*list(base_model_SqueezeNet.children())[:-1])  # Remove the last classification layer\n",
        "# Function to extract features\n",
        "def extract_features_SqueezeNet(dataloader, model):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    for images, labels in dataloader:\n",
        "        with torch.no_grad():\n",
        "            features = model(images)\n",
        "            features_flat = features.view(features.size(0), -1)\n",
        "            features_list.append(features_flat.numpy())\n",
        "            labels_list.append(labels.numpy())\n",
        "    features = np.vstack(features_list)\n",
        "    labels = np.hstack(labels_list)\n",
        "    return features, labels\n",
        "# Extract features from the sample dataset\n",
        "sample_features_SqueezeNet, sample_labels_SqueezeNet = extract_features_SqueezeNet(\n",
        "    dataloader_SqueezeNet,\n",
        "    model_SqueezeNet\n",
        ")\n",
        "sample_data_SqueezeNet = np.column_stack((sample_features_SqueezeNet, sample_labels_SqueezeNet)) # Combine features and labels\n",
        "# Create a DataFrame\n",
        "columns_SqueezeNet = [f'feature_{i}' for i in range(sample_features_SqueezeNet.shape[1])] + ['class']\n",
        "sample_df_SqueezeNet = pd.DataFrame(sample_data_SqueezeNet, columns=columns_SqueezeNet)\n",
        "\n",
        "sample_df_SqueezeNet.to_csv('sample_features_squeezenet_pytorch.csv', index=False) # Save to CSV\n",
        "print(\"Feature extraction complete. CSV file has been generated as 'sample_features_squeezenet_pytorch.csv'.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}